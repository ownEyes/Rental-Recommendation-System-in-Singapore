{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add required cols for frontend display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "processed_data = pd.read_csv('./dataSource/processed_data_merged.csv')\n",
    "merged_data=pd.read_csv('./dataSource/data_merged(final+version).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_count(df):\n",
    "    print(\"Shape before delete:\", df.shape)\n",
    "    df_cleaned = df.dropna()\n",
    "    print(\"Shape after delete:\", df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_names = processed_data[merged_data['name'].isnull() | (merged_data['name'] == '')]\n",
    "\n",
    "# 打印结果\n",
    "empty_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['name']= merged_data['name']+' in '+merged_data['neighbourhood_cleansed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_nan = merged_data.isna().any()\n",
    "\n",
    "# 提取包含缺失值的列\n",
    "data_with_nan = merged_data[cols_with_nan[cols_with_nan].index]\n",
    "data_with_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_merge = ['name', 'details', 'accommodates', 'picture_url',\n",
    "                   'review_scores_rating', 'review_scores_accuracy',\n",
    "                   'review_scores_cleanliness', 'review_scores_checkin',\n",
    "                   'review_scores_communication', 'review_scores_location',\n",
    "                   'review_scores_value', 'listing_url']\n",
    "merged_data['details'] = merged_data['name'].str.split(' · ',n=2).str[2]\n",
    "merged_data['name'] = merged_data['name'].str.split(' in ').str[0]\n",
    "merged_data['name']= merged_data['name']+' in '+merged_data['neighbourhood_cleansed']\n",
    "\n",
    "processed_data = processed_data.merge(merged_data[['id'] + columns_to_merge],\n",
    "                                       on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3438</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3439</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3440</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3441</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3442</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3441 rows × 0 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
       "\n",
       "[3441 rows x 0 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_with_nan = processed_data.isna().any()\n",
    "\n",
    "# 提取包含缺失值的列\n",
    "data_with_nan = processed_data[cols_with_nan[cols_with_nan].index]\n",
    "data_with_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = processed_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>neighbourhood_cleansed</th>\n",
       "      <th>neighbourhood_group_cleansed</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>room_type</th>\n",
       "      <th>price</th>\n",
       "      <th>minimum_months</th>\n",
       "      <th>maximum_months</th>\n",
       "      <th>distance_to_mrt</th>\n",
       "      <th>...</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>picture_url</th>\n",
       "      <th>review_scores_rating</th>\n",
       "      <th>review_scores_accuracy</th>\n",
       "      <th>review_scores_cleanliness</th>\n",
       "      <th>review_scores_checkin</th>\n",
       "      <th>review_scores_communication</th>\n",
       "      <th>review_scores_location</th>\n",
       "      <th>review_scores_value</th>\n",
       "      <th>listing_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, neighbourhood_cleansed, neighbourhood_group_cleansed, latitude, longitude, room_type, price, minimum_months, maximum_months, distance_to_mrt, closest_mrt_name, closest_mrt_stop_id, closest_mall_distance, closest_mall_name, closest_mall_address, conditioning, BBQ, gym, pool, dryer, Wifi, kitchen, Backyard, TV, refrigerator, Microwave, Oven, Pets, stove, fan, name, details, accommodates, picture_url, review_scores_rating, review_scores_accuracy, review_scores_cleanliness, review_scores_checkin, review_scores_communication, review_scores_location, review_scores_value, listing_url]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 42 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_details = processed_data[processed_data['details'].isnull() | (processed_data['details'] == '')]\n",
    "\n",
    "# 打印结果\n",
    "empty_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = processed_data[~(processed_data['details'].isnull() | (processed_data['details'] == ''))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.to_csv('./dataSource/final_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "processed_data = pd.read_csv('./dataSource/processed_data_merged.csv')\n",
    "merged_data=pd.read_csv('./dataSource/data_merged(final+version).csv')\n",
    "\n",
    "# def replace_value(row):\n",
    "#     if 'Singapore' in row['name']:\n",
    "#         return row['name'].replace('Singapore', row['neighbourhood_cleansed'])\n",
    "#     else:\n",
    "#         return row['name']\n",
    "    \n",
    "\n",
    "# merged_data['name'] = merged_data['name'].str.split('Singapore').str[0]\n",
    "column1_to_add = merged_data['name'].str.split(' · ',n=2).str[2]\n",
    "\n",
    "merged_data['name'] = merged_data['name'].str.split(' in ').str[0]\n",
    "merged_data['name']= merged_data['name']+' in '+merged_data['neighbourhood_cleansed']\n",
    "\n",
    "column2_to_add=merged_data['name']\n",
    "column3_to_add=merged_data['accommodates']\n",
    "column4_to_add=merged_data['picture_url']\n",
    "\n",
    "processed_data.insert(1, 'details', column1_to_add)\n",
    "processed_data.insert(1, 'name', column2_to_add)\n",
    "processed_data.insert(16, 'accommodates', column3_to_add)\n",
    "processed_data.insert(5, 'picture_url', column4_to_add)\n",
    "\n",
    "processed_data.to_csv('processed_data_merged_final_v1.csv', index=False)\n",
    "\n",
    "new=pd.read_csv('./csv_output/processed_data_merged_final_v1.csv')\n",
    "new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delete row with empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('./csv_output/processed_data_merged_final_v1.csv')\n",
    "\n",
    "print(\"Shape before delete:\", df.shape)\n",
    "df_cleaned = df.dropna()\n",
    "print(\"Shape after delete:\", df_cleaned.shape)\n",
    "df_cleaned.to_csv('./csv_output/processed_data_merged_final_v2.csv', index=False)\n",
    "\n",
    "df_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add rating cols for default rating-based recommdation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df1 = pd.read_csv('./csv_output/processed_data_merged_final_v2.csv')\n",
    "df2=pd.read_csv('./dataSource/data_merged(final+version).csv',\n",
    "                usecols=['review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value'])\n",
    "df=pd.concat([df1, df2], axis=1)\n",
    "\n",
    "df.to_csv('./csv_output/processed_data_merged_final_v3.csv', index=False)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add room details for Linze 5002 regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "original = pd.read_csv('./csv_output/processed_data_merged_final_v2.csv')\n",
    "\n",
    "type_to_add=original['name']\n",
    "type_to_add = type_to_add.str.split(' in ').str[0]\n",
    "\n",
    "studio_to_add=original['details']\n",
    "studio_to_add = studio_to_add.apply(lambda x: 1 if 'Studio' in x else 0)\n",
    "\n",
    "shared_bath_to_add=original['details']\n",
    "shared_bath_to_add=shared_bath_to_add.apply(lambda x: 0.5 if 'Shared half-bath' in x else 0)\n",
    "\n",
    "private_bath_to_add=original['details']\n",
    "private_bath_to_add=private_bath_to_add.apply(lambda x: 0.5 if 'Private half-bath' in x else 0)\n",
    "\n",
    "def parse_room_info(info):\n",
    "    counts = {}  \n",
    "\n",
    "\n",
    "    matches = re.findall(r'(\\d+(\\.\\d+)?)\\s*([\\w\\s]+)\\b', info)\n",
    "    for count, _, room_type in matches:\n",
    "        counts[room_type.strip()] = float(count)\n",
    "\n",
    "    return counts\n",
    "\n",
    "parsed_room_info = original['details'].apply(parse_room_info)\n",
    "\n",
    "parsed_df = pd.DataFrame(parsed_room_info.tolist(), index=original.index)\n",
    "\n",
    "\n",
    "df = pd.concat([original, parsed_df], axis=1)\n",
    "df.insert(1, 'types', type_to_add)\n",
    "\n",
    "df[['bedrooms', 'bedroom', 'beds', 'bed', 'shared baths','private bath', 'shared bath', 'baths', 'bath']] = df[['bedrooms', 'bedroom', 'beds', 'bed', 'shared baths','private bath', 'shared bath', 'baths', 'bath']].fillna(0)\n",
    "df['total_bedrooms'] = df['bedrooms'] + df['bedroom']\n",
    "df['total_beds'] = df['beds'] + df['bed']\n",
    "df['total_shared_baths'] = df['shared baths'] + df['shared bath']+shared_bath_to_add\n",
    "\n",
    "df['private bath']=df['private bath']+ private_bath_to_add\n",
    "df['total_baths'] = df['baths'] + df['bath']\n",
    "\n",
    "def get_bath_type(row):\n",
    "    if row['private bath'] != 0:\n",
    "        return 'private'\n",
    "    elif row['total_shared_baths'] != 0:\n",
    "        return 'shared'\n",
    "    elif row['total_baths'] != 0:\n",
    "        return 'unknown'\n",
    "    else:\n",
    "        return 'none'\n",
    "\n",
    "df['bath_type'] = df.apply(get_bath_type, axis=1)\n",
    "\n",
    "df['total_baths'] =df['total_baths'] +df['total_shared_baths']+ df['private bath']\n",
    "\n",
    "\n",
    "df = df.drop(columns=['bedrooms', 'bedroom', 'beds', 'bed', 'shared baths', 'shared bath', 'baths', 'bath','total_shared_baths','private bath'])\n",
    "df.insert(loc=len(df.columns), column='studio', value=studio_to_add)\n",
    "\n",
    "df.to_csv('./csv_output/data_merged_5002_v2.csv', index=False)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw=pd.read_csv('./dataSource/reviews.csv')\n",
    "nan_count(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating=pd.read_csv('./csv_output/ratingInfo.csv')\n",
    "nan_count(rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Followings for rating estimation from review comments\n",
    "### Delete cols not in en language, check comments by same users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langdetect import detect_langs\n",
    "\n",
    "raw=pd.read_csv('./dataSource/reviews.csv')\n",
    "\n",
    "duplicate_ids = raw['reviewer_id'].duplicated()\n",
    "\n",
    "\n",
    "duplicate=raw[duplicate_ids]\n",
    "duplicate.head()\n",
    "\n",
    "# raw = raw.drop_duplicates(subset=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows:\", duplicate.shape[0])  \n",
    "print(\"Number of columns:\", duplicate.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_ids = raw['reviewer_name'].duplicated()\n",
    "\n",
    "\n",
    "duplicate=raw[duplicate_ids]\n",
    "duplicate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows:\", duplicate.shape[0])  \n",
    "print(\"Number of columns:\", duplicate.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### and drap cols which is not useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = raw.drop(columns=['date', 'id'])\n",
    "\n",
    "def is_all_english_text(text):\n",
    "    try:\n",
    "        langs = detect_langs(text)\n",
    "        for lang in langs:\n",
    "            if lang.lang != 'en':\n",
    "                return False\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "raw = raw[raw['comments'].apply(is_all_english_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_ids = raw['reviewer_id'].duplicated()\n",
    "\n",
    "\n",
    "duplicate=raw[duplicate_ids]\n",
    "print(\"Number of rows:\", duplicate.shape[0])  \n",
    "print(\"Number of columns:\", duplicate.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### asign new user ID for database construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['userID'] = pd.factorize(raw['reviewer_id'])[0] + 1\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save user and their comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import re\n",
    "\n",
    "raw = raw.drop(columns=['reviewer_id'])\n",
    "raw_reset = raw.reset_index(drop=True)\n",
    "# raw_reset['comments'] = raw_reset['comments'].str.replace('<br/>', '\\n')\n",
    "# raw_reset['comments'] = raw_reset['comments'].str.replace('\\n', ' ')\n",
    "\n",
    "raw_reset['comments'] = raw_reset['comments'].apply(lambda x: re.sub(r'<br/>', '\\n', x))\n",
    "raw_reset['comments'] = raw_reset['comments'].apply(lambda x: re.sub(r'\\n+', '\\n', x))\n",
    "\n",
    "commentsInfo=raw_reset[['listing_id','userID','comments']]\n",
    "\n",
    "commentsInfo.to_csv('./csv_output/commentsInfo.csv', index=False)\n",
    "\n",
    "commentsInfo.head()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keep unquie user id, generate unquie username, pwd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userInfo=raw_reset[['userID','reviewer_name']]\n",
    "userInfo = userInfo.rename(columns={'reviewer_name': 'userName'})\n",
    "userInfo = userInfo.drop_duplicates(subset=['userID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### need delete special characters in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_username(username):\n",
    "    pattern = re.compile(r'[^a-zA-Z0-9_]+')\n",
    "    return re.sub(pattern, '_', username)\n",
    "\n",
    "\n",
    "userInfo['userName'] = userInfo['userName'].apply(process_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "used_usernames = set()\n",
    "\n",
    "for index, row in userInfo.iterrows():\n",
    "    username = row['userName']\n",
    "    \n",
    "    if username in used_usernames:\n",
    "        while True:\n",
    "            new_username = f\"{username}_{random.randint(1, 999)}\"\n",
    "            if new_username not in used_usernames and len(new_username) <= 20:\n",
    "                break\n",
    "        userInfo.at[index, 'userName'] = new_username\n",
    "        used_usernames.add(new_username)\n",
    "    else:\n",
    "        used_usernames.add(username)\n",
    "\n",
    "def generate_password(length):\n",
    "    characters = string.ascii_letters + string.digits + string.punctuation\n",
    "    password = ''.join(random.choice(characters) for _ in range(length))\n",
    "    return password\n",
    "\n",
    "\n",
    "userInfo['password'] = userInfo.apply(lambda row: generate_password(random.randint(6, 20)), axis=1)\n",
    "userInfo.to_csv('./csv_output/userInfo.csv', index=False)\n",
    "\n",
    "userInfo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check unquieness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_ids = userInfo['userName'].duplicated()\n",
    "\n",
    "\n",
    "duplicate=userInfo[duplicate_ids]\n",
    "duplicate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows:\", duplicate.shape[0])  \n",
    "print(\"Number of columns:\", duplicate.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_ids = userInfo['userID'].duplicated()\n",
    "\n",
    "\n",
    "duplicate=userInfo[duplicate_ids]\n",
    "duplicate.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rating estimation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from flair.nn import Classifier\n",
    "from flair.data import Sentence\n",
    "import pandas as pd\n",
    "\n",
    "comments=pd.read_csv('./csv_output/commentsInfo.csv')\n",
    "\n",
    "classifier = Classifier.load('sentiment')\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid_mapping(score):\n",
    "    mapped_score = 5 / (1 + math.exp(-score))\n",
    "    return mapped_score\n",
    "\n",
    "def tanh_mapping(score):\n",
    "    mapped_score = 2.5 * math.tanh(score) + 2.5\n",
    "    return mapped_score\n",
    "\n",
    "# def analyze_sentiment(text):\n",
    "#     sentence = Sentence(text)\n",
    "#     classifier.predict(sentence)\n",
    "\n",
    "\n",
    "#     positivity_score = sentence.labels[0].score\n",
    "#     mapped_positivity_score = tanh_mapping(positivity_score)\n",
    "\n",
    "#     estimated_sentiment=sentence.labels[0].value\n",
    "#     # estimated_rating = sigmoid_mapping(positivity_score) if sentence.labels[0].value == 'POSITIVE' else 5 - sigmoid_mapping(positivity_score)\n",
    "#     estimated_rating = mapped_positivity_score if estimated_sentiment == 'POSITIVE' else 5 - mapped_positivity_score\n",
    "    \n",
    "#     return estimated_rating,estimated_sentiment\n",
    "\n",
    "\n",
    "# def exponential_mapping(score, sentiment_value):\n",
    "#     a=7\n",
    "\n",
    "#     normalized_score = (score - 0.9) / 0.1  \n",
    "\n",
    "#     center = 4.5 if sentiment_value == 'POSITIVE' else 0.5\n",
    "\n",
    "#     mapped_score = math.exp((normalized_score - center) * a)  \n",
    "\n",
    "#     mapped_score = mapped_score * 5.0\n",
    "#     return mapped_score\n",
    "import math\n",
    "\n",
    "def exponential_mapping(score, sentiment_value):\n",
    "    a = 2.0  \n",
    "\n",
    "\n",
    "    normalized_score = max(0, min(1, (score - 0.5) / 0.5))\n",
    "\n",
    "\n",
    "    center = 0.99 if sentiment_value == 'POSITIVE' else 0.01\n",
    "\n",
    "\n",
    "    mapped_score = math.exp(-a * abs(normalized_score - center))\n",
    "\n",
    "    mapped_score = mapped_score * 5.0\n",
    "\n",
    "    return mapped_score\n",
    "\n",
    "\n",
    "def analyze_sentiment_flair(text):\n",
    "    sentence = Sentence(text)\n",
    "    classifier.predict(sentence)\n",
    "\n",
    "    positivity_score = sentence.labels[0].score\n",
    "    sentiment_value = sentence.labels[0].value\n",
    "\n",
    "    estimated_rating = exponential_mapping(positivity_score, sentiment_value)\n",
    "\n",
    "    return estimated_rating, sentiment_value,positivity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments[['estimated_rating_flair','estimated_sentiment_flair','probability']] = comments['comments'].apply(lambda x: pd.Series(analyze_sentiment_flair(x)))\n",
    "comments.head()\n",
    "comments.to_csv('./csv_output/commentsInfo_flair_exp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "comments=pd.read_csv('./csv_output/commentsInfo_flair_exp.csv')\n",
    "\n",
    "def analyze_sentiment_textblob(text):\n",
    "    testimonial = TextBlob(text)\n",
    "    \n",
    "    sentiment_value = testimonial.sentiment.polarity\n",
    "    subjectivity_value = testimonial.sentiment.subjectivity\n",
    "    estimated_rating=(testimonial.sentiment.polarity+1)*2.5\n",
    "    return estimated_rating,sentiment_value,subjectivity_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments[['estimated_rating_textblob','estimated_sentiment_textblob','estimated_subjectivity_textblob']] = comments['comments'].apply(lambda x: pd.Series(analyze_sentiment_textblob(x)))\n",
    "comments.head()\n",
    "comments.to_csv('./csv_output/commentsInfo_flair_textblob.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import asent\n",
    "comments=pd.read_csv('./csv_output/commentsInfo_flair_textblob.csv')\n",
    "# create spacy pipeline\n",
    "nlp = spacy.blank('en')\n",
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "# add the rule-based sentiment model\n",
    "nlp.add_pipe(\"asent_en_v1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_asent(nlp,text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    neg_value = doc._.polarity.negative\n",
    "    neu_value = doc._.polarity.neutral\n",
    "    pos_value = doc._.polarity.positive\n",
    "    compound_value=doc._.polarity.compound\n",
    "    # estimated_rating=sentiment_mapping(neg_value,neu_value,pos_value,compound_value)\n",
    "    # return neg_value,neu_value,pos_value,compound_value,estimated_rating\n",
    "    return neg_value,neu_value,pos_value,compound_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments[['neg_value_asent','neu_value_asent','pos_value_asent','compound_value_asent','estimated_rating_asent']] = comments['comments'].apply(lambda x: pd.Series(analyze_sentiment_asent(nlp,x)))\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.to_csv('./csv_output/commentsInfo_flair_textblob_asent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacytextblob\n",
    "!python -m textblob.download_corpora\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "# comments=pd.read_csv('./csv_output/commentsInfo_flair_textblob_asent.csv')\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# nlp.add_pipe(\"spacytextblob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def analyze_sentiment_spacytextblob(nlp,text):\n",
    "#     doc = nlp(text)\n",
    "    \n",
    "#     sentiment_value = doc._.blob.polarity\n",
    "#     estimated_rating=(doc._.blob.polarity+1)*2.5\n",
    "#     return estimated_rating,sentiment_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments[['estimated_rating_spacytextblob','estimated_sentiment_spacytextblob']] = comments['comments'].apply(lambda x: pd.Series(analyze_sentiment_spacytextblob(nlp,x)))\n",
    "# comments.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments.to_csv('./csv_output/commentsInfo_flair_textblob_asent_spacytextblob.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('./csv_output/commentsInfo_flair_textblob_asent.csv')\n",
    "\n",
    "\n",
    "df = df[~df['comments'].str.contains('This is an automated posting.')]\n",
    "\n",
    "\n",
    "df.to_csv('./csv_output/commentsInfo_flair_textblob_asent_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different_rows = df[df['estimated_sentiment_textblob'] != df['estimated_sentiment_spacytextblob']]\n",
    "# different_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob=pd.read_csv('./csv_output/commentsInfo_flair_textblob_asent_clean.csv')\n",
    "ob['estimated_rating_textblob']=ob['estimated_rating_textblob']\n",
    "obj=ob[['comments','estimated_rating_flair','estimated_rating_textblob','estimated_rating_asent','pos_value_asent','neg_value_asent','probability','estimated_sentiment_flair','estimated_sentiment_textblob','compound_value_asent','neu_value_asent','estimated_subjectivity_textblob']]\n",
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.to_csv('./csv_output/rating_observations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw=pd.read_csv('./csv_output/rating_observations.csv')\n",
    "\n",
    "# def map_sentiment(sentiment):\n",
    "#     return 1 if sentiment == 'POSITIVE' else -1\n",
    "\n",
    "# raw['sentiment_mapping'] = raw['estimated_sentiment_flair'].apply(map_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition1 = raw['estimated_sentiment_flair'].apply(lambda x: 1 if x == 'POSITIVE' else -1)\n",
    "condition2 = raw['estimated_sentiment_textblob'].apply(lambda x: 1 if x > 0 else -1)\n",
    "condition3 = raw['compound_value_asent'].apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "different_signs_condition = (condition1 != condition2) | (condition2 != condition3)\n",
    "result_df = raw[different_signs_condition]\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('./csv_output/observe_diff.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_csv(\"./csv_output/DT_rule.csv\")\n",
    "\n",
    "le_result = LabelEncoder()\n",
    "data['result'] = le_result.fit_transform(data['result'])\n",
    "\n",
    "le_flair = LabelEncoder()\n",
    "data['estimated_sentiment_flair'] = le_flair.fit_transform(data['estimated_sentiment_flair'])\n",
    "\n",
    "# 选择输入变量和目标变量\n",
    "X = data[['pos_value_asent','neg_value_asent','probability','estimated_sentiment_flair','estimated_sentiment_textblob','compound_value_asent','neu_value_asent','estimated_subjectivity_textblob']]\n",
    "y = data['result']\n",
    "\n",
    "# 初始化决策树模型\n",
    "clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "\n",
    "# 进行5折交叉验证\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "\n",
    "print(\"Accuracy scores for each fold: \", scores)\n",
    "print(\"Average accuracy: \", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_csv(\"./csv_output/DT_rule.csv\")\n",
    "\n",
    "# 将'POSITIVE', 'NEGATIVE', 和'NEUTRAL'转换为数值\n",
    "le_result = LabelEncoder()\n",
    "data['result'] = le_result.fit_transform(data['result'])\n",
    "\n",
    "le_flair = LabelEncoder()\n",
    "data['estimated_sentiment_flair'] = le_flair.fit_transform(data['estimated_sentiment_flair'])\n",
    "\n",
    "# 选择输入变量和目标变量\n",
    "X = data[['pos_value_asent','neg_value_asent','probability','estimated_sentiment_flair','estimated_sentiment_textblob','compound_value_asent','neu_value_asent','estimated_subjectivity_textblob']]\n",
    "y = data['result']\n",
    "\n",
    "# 初始化决策树模型\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# 定义要搜索的参数网格\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4,5],\n",
    "    'min_samples_split': [2,3,4, 5,6,7,8,9, 10],\n",
    "    'min_samples_leaf': [1, 2, 3,4]\n",
    "}\n",
    "\n",
    "# 使用GridSearchCV进行搜索\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# 打印最佳参数和相应的准确率\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# 保存最佳模型\n",
    "joblib.dump(grid_search.best_estimator_, './fileOutput/best_decision_tree_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_csv(\"./csv_output/DT_rule.csv\")\n",
    "\n",
    "# 将'POSITIVE', 'NEGATIVE', 和'NEUTRAL'转换为数值\n",
    "le_result = LabelEncoder()\n",
    "data['result'] = le_result.fit_transform(data['result'])\n",
    "\n",
    "le_flair = LabelEncoder()\n",
    "data['estimated_sentiment_flair'] = le_flair.fit_transform(data['estimated_sentiment_flair'])\n",
    "\n",
    "# 选择输入变量和目标变量\n",
    "X = data[['pos_value_asent','neg_value_asent','probability','estimated_sentiment_flair','estimated_sentiment_textblob','compound_value_asent','neu_value_asent','estimated_subjectivity_textblob']]\n",
    "y = data['result']\n",
    "\n",
    "# 初始化随机森林模型\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# 定义要搜索的参数网格\n",
    "# param_grid = {\n",
    "#     'n_estimators': [10, 50, 100, 200],\n",
    "#     'max_depth': [3, 4, 5],\n",
    "#     'min_samples_split': [2, 3, 4, 5],\n",
    "#     'min_samples_leaf': [1, 2, 3, 4]\n",
    "# }\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [5,10,15, 20,30,40,50,100, 200],\n",
    "    'max_depth': [1, 2, 3,None],\n",
    "    'min_samples_split': [2,3,4, 5,6,7,8,9, 10],\n",
    "    'min_samples_leaf': [1, 2,3,4, 5,6,7,8,9, 10],\n",
    "    'bootstrap':[True,False],\n",
    "    'max_features':['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# 使用GridSearchCV进行搜索\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# 打印最佳参数和相应的准确率\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "# 保存最佳模型\n",
    "joblib.dump(grid_search.best_estimator_, './fileOutput/best_random_forest_model.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final comments and user dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langdetect import detect_langs\n",
    "\n",
    "raw=pd.read_csv('./dataSource/reviews.csv')\n",
    "raw = raw.drop(columns=['date', 'id'])\n",
    "\n",
    "def is_all_english_text(text):\n",
    "    try:\n",
    "        langs = detect_langs(text)\n",
    "        for lang in langs:\n",
    "            if lang.lang != 'en':\n",
    "                return False\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "raw = raw[raw['comments'].apply(is_all_english_text)]\n",
    "raw = raw[~raw['comments'].str.contains('This is an automated posting.')]\n",
    "\n",
    "raw['userID'] = pd.factorize(raw['reviewer_id'])[0] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import re\n",
    "\n",
    "raw = raw.drop(columns=['reviewer_id'])\n",
    "raw_reset = raw.reset_index(drop=True)\n",
    "\n",
    "\n",
    "raw_reset['comments'] = raw_reset['comments'].apply(lambda x: re.sub(r'<br/>', '\\n', x))\n",
    "raw_reset['comments'] = raw_reset['comments'].apply(lambda x: re.sub(r'\\n+', '\\n', x))\n",
    "\n",
    "commentsInfo=raw_reset[['listing_id','userID','comments']]\n",
    "\n",
    "commentsInfo.to_csv('./csv_output/commentsInfo.csv', index=False)\n",
    "\n",
    "commentsInfo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userInfo=raw_reset[['userID','reviewer_name']]\n",
    "userInfo = userInfo.rename(columns={'reviewer_name': 'userName'})\n",
    "userInfo = userInfo.drop_duplicates(subset=['userID'])\n",
    "\n",
    "def process_username(username):\n",
    "    pattern = re.compile(r'[^a-zA-Z0-9_]+')\n",
    "    return re.sub(pattern, '_', username)\n",
    "\n",
    "\n",
    "userInfo['userName'] = userInfo['userName'].apply(process_username)\n",
    "\n",
    "used_usernames = set()\n",
    "\n",
    "for index, row in userInfo.iterrows():\n",
    "    username = row['userName']\n",
    "    \n",
    "    if username in used_usernames:\n",
    "        while True:\n",
    "            new_username = f\"{username}_{random.randint(1, 999)}\"\n",
    "            if new_username not in used_usernames and len(new_username) <= 20:\n",
    "                break\n",
    "        userInfo.at[index, 'userName'] = new_username\n",
    "        used_usernames.add(new_username)\n",
    "    else:\n",
    "        used_usernames.add(username)\n",
    "\n",
    "def generate_password(length):\n",
    "    characters = string.ascii_letters + string.digits + string.punctuation\n",
    "    password = ''.join(random.choice(characters) for _ in range(length))\n",
    "    return password\n",
    "\n",
    "\n",
    "userInfo['password'] = userInfo.apply(lambda row: generate_password(random.randint(6, 20)), axis=1)\n",
    "userInfo.to_csv('./csv_output/userInfo.csv', index=False)\n",
    "\n",
    "userInfo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_ids = userInfo['userName'].duplicated()\n",
    "\n",
    "\n",
    "duplicate=userInfo[duplicate_ids]\n",
    "duplicate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_ids = userInfo['userID'].duplicated()\n",
    "\n",
    "\n",
    "duplicate=userInfo[duplicate_ids]\n",
    "duplicate.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final rating estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from flair.nn import Classifier\n",
    "from flair.data import Sentence\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.special import expit, erf\n",
    "import joblib\n",
    "import spacy\n",
    "import asent\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_mapping(score):\n",
    "    mapped_score = 5 / (1 + math.exp(-score))\n",
    "    return mapped_score\n",
    "\n",
    "def tanh_mapping(score):\n",
    "    mapped_score = 2.5 * math.tanh(score) + 2.5\n",
    "    return mapped_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('./fileOutput/best_random_forest_model.joblib')\n",
    "comments=pd.read_csv('./csv_output/commentsInfo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('/content/drive/My Drive/gp/best_random_forest_model.joblib')\n",
    "comments=pd.read_csv('/content/drive/My Drive/gp/commentsInfo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_analysis_init():\n",
    "    classifier = Classifier.load('sentiment')\n",
    "    nlp = spacy.blank('en')\n",
    "    nlp.add_pipe('sentencizer')\n",
    "\n",
    "    # add the rule-based sentiment model\n",
    "    nlp.add_pipe(\"asent_en_v1\")\n",
    "    return classifier,nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_mapping(score, sentiment_value):\n",
    "    a = 2.0  \n",
    "\n",
    "\n",
    "    normalized_score = max(0, min(1, (score - 0.5) / 0.5))\n",
    "\n",
    "\n",
    "    center = 0.99 if sentiment_value == 'POSITIVE' else 0.01\n",
    "\n",
    "\n",
    "    mapped_score = math.exp(-a * abs(normalized_score - center))\n",
    "\n",
    "    mapped_score = mapped_score * 5.0\n",
    "\n",
    "    return mapped_score\n",
    "\n",
    "\n",
    "def analyze_sentiment_flair(classifier,text):\n",
    "    sentence = Sentence(text)\n",
    "    classifier.predict(sentence)\n",
    "\n",
    "    positivity_score = sentence.labels[0].score\n",
    "    sentiment_value = sentence.labels[0].value\n",
    "\n",
    "    estimated_rating = exponential_mapping(positivity_score, sentiment_value)\n",
    "\n",
    "    return estimated_rating, sentiment_value,positivity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_textblob(text):\n",
    "    testimonial = TextBlob(text)\n",
    "    \n",
    "    sentiment_value = testimonial.sentiment.polarity\n",
    "    subjectivity_value = testimonial.sentiment.subjectivity\n",
    "    # estimated_rating=(testimonial.sentiment.polarity+1)*2.5\n",
    "    # return estimated_rating,sentiment_value,subjectivity_value\n",
    "    return sentiment_value,subjectivity_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sentiment_mapping(neg,neu,pos,compound):\n",
    "    neg_weight=5\n",
    "    neu_weight=10\n",
    "    pos_weight=5\n",
    "    # composite_score = - (neg_weight * neg) + (neu_weight * neu) + (pos_weight * pos)\n",
    "    composite_score=(pos_weight * pos + neu_weight * neu - neg_weight * neg) / (pos_weight + neu_weight + neg_weight)\n",
    "    composite_score=erf(composite_score)\n",
    "    mapped_score=5.0*expit((composite_score+(compound+1)*0.5)/2)\n",
    "\n",
    "    return mapped_score\n",
    "\n",
    "\n",
    "def analyze_sentiment_asent(nlp,text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    neg_value = doc._.polarity.negative\n",
    "    neu_value = doc._.polarity.neutral\n",
    "    pos_value = doc._.polarity.positive\n",
    "    compound_value=doc._.polarity.compound\n",
    "    # estimated_rating=sentiment_mapping(neg_value,neu_value,pos_value,compound_value)\n",
    "    # return neg_value,neu_value,pos_value,compound_value,estimated_rating\n",
    "    return neg_value,neu_value,pos_value,compound_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(classifier,nlp,text):\n",
    "    \n",
    "    estimated_rating, sentiment_flair,positivity_flair=analyze_sentiment_flair(classifier,text)\n",
    "    sentiment_textblob,subjectivity_textblob=analyze_sentiment_textblob(text)\n",
    "    neg_asent,neu_asent,pos_asent,compound_asent=analyze_sentiment_asent(nlp,text)\n",
    "   \n",
    "    return estimated_rating,pos_asent,neg_asent,positivity_flair,sentiment_flair,sentiment_textblob,compound_asent,neu_asent,subjectivity_textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def rating_estimation(model,estimated_rating,pos_asent,neg_asent,positivity_flair,sentiment_flair,sentiment_textblob,compound_asent,neu_asent,subjectivity_textblob):\n",
    "#     rating_list=[2,2.5,3]\n",
    "#     sentiment_list=['NEGATIVE','POSITIVE']\n",
    "#     if positivity_flair>=0.9:\n",
    "#         return estimated_rating\n",
    "#     else:\n",
    "#         sentiment_flair = sentiment_list.index(sentiment_flair)\n",
    "#         feature_names=['pos_value_asent','neg_value_asent','probability','estimated_sentiment_flair','estimated_sentiment_textblob','compound_value_asent','neu_value_asent','estimated_subjectivity_textblob']\n",
    "#         model_input=np.array([[pos_asent,neg_asent,positivity_flair,sentiment_flair,sentiment_textblob,compound_asent,neu_asent,subjectivity_textblob]])\n",
    "#         model_input_df = pd.DataFrame(model_input, columns=feature_names)\n",
    "#         estimated_rating=rating_list[model.predict(model_input_df)[0]]\n",
    "#     return estimated_rating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_estimation(model, data):\n",
    "    rating_list = [2, 2.5, 3]\n",
    "    sentiment_list = ['NEGATIVE', 'POSITIVE']\n",
    "    \n",
    "    # 将情感从文本转换为数字\n",
    "    data['sentiment_flair'] = data['sentiment_flair'].apply(lambda x: sentiment_list.index(x) if x in sentiment_list else x)\n",
    "    \n",
    "    # 选择所有正向的情感，并直接给出评分\n",
    "    positive_mask = data['positivity_flair'] >= 0.9\n",
    "    ratings = data.loc[positive_mask, 'estimated_rating']\n",
    "    \n",
    "    # 选择所有非正向的情感，使用模型进行预测\n",
    "    not_positive_mask = ~positive_mask\n",
    "    not_positive_data = data.loc[not_positive_mask]\n",
    "    if not not_positive_data.empty:\n",
    "        feature_names = ['pos_value_asent', 'neg_value_asent', 'probability', 'estimated_sentiment_flair', 'estimated_sentiment_textblob', 'compound_value_asent', 'neu_value_asent', 'estimated_subjectivity_textblob']\n",
    "        cols=['pos_asent','neg_asent','positivity_flair','sentiment_flair','sentiment_textblob','compound_asent','neu_asent','subjectivity_textblob']\n",
    "        model_input = not_positive_data[cols]\n",
    "        model_input.columns=feature_names\n",
    "        model_predictions = model.predict(model_input)\n",
    "        model_ratings = pd.Series([rating_list[pred] for pred in model_predictions], index=not_positive_data.index)\n",
    "        ratings = pd.concat([ratings, model_ratings])\n",
    "        \n",
    "    # 将评分按照原始的索引排序\n",
    "    ratings = ratings.sort_index()\n",
    "    \n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier,nlp=comment_analysis_init()\n",
    "\n",
    "temp_df = comments['comments'].apply(lambda x: pd.Series(analyze_sentiment_flair(classifier,x)))\n",
    "temp_df.columns = ['estimated_rating', 'sentiment_flair','positivity_flair']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df[['sentiment_textblob','subjectivity_textblob']] = comments['comments'].apply(lambda x: pd.Series(analyze_sentiment_textblob(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df[['neg_asent','neu_asent','pos_asent','compound_asent']] = comments['comments'].apply(lambda x: pd.Series(analyze_sentiment_asent(nlp,x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = temp_df[['estimated_rating','pos_asent', 'neg_asent', 'positivity_flair', 'sentiment_flair', 'sentiment_textblob', 'compound_asent', 'neu_asent', 'subjectivity_textblob']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments['rating'] = temp_df.apply(lambda row: rating_estimation(model, row['estimated_rating'], row['pos_asent'], row['neg_asent'], row['positivity_flair'], row['sentiment_flair'], row['sentiment_textblob'], row['compound_asent'], row['neu_asent'], row['subjectivity_textblob']), axis=1)\n",
    "comments['rating'] = rating_estimation(model, temp_df)\n",
    "del temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.to_csv('/content/drive/My Drive/gp/ratingInfo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "model = joblib.load('./fileOutput/best_random_forest_model.joblib')\n",
    "temp_df=pd.read_csv('./csv_output/temp_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.columns=['pos_asent', 'neg_asent', 'positivity_flair', 'sentiment_flair', 'sentiment_textblob', 'compound_asent', 'neu_asent', 'subjectivity_textblob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['rating'] = rating_estimation(model, temp_df)\n",
    "del temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.to_csv('./csv_output/ratingInfo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('./dataSource/final_data.csv')\n",
    "df=df.drop('Unnamed: 0', axis=1)\n",
    "df.to_csv('./dataSource/final_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./csv_output/poiInfo.csv')\n",
    "df=df.drop('Unnamed: 0', axis=1)\n",
    "df.to_csv('./csv_output/poiInfo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>food</th>\n",
       "      <th>health</th>\n",
       "      <th>restaurant</th>\n",
       "      <th>hospital</th>\n",
       "      <th>lodging</th>\n",
       "      <th>finance</th>\n",
       "      <th>cafe</th>\n",
       "      <th>convenience_store</th>\n",
       "      <th>clothing_store</th>\n",
       "      <th>...</th>\n",
       "      <th>police</th>\n",
       "      <th>light_rail_station</th>\n",
       "      <th>city_hall</th>\n",
       "      <th>train_station</th>\n",
       "      <th>natural_feature</th>\n",
       "      <th>subpremise</th>\n",
       "      <th>name</th>\n",
       "      <th>formatted_address</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Quayside Isle</td>\n",
       "      <td>31 Ocean Way, Singapore 098375</td>\n",
       "      <td>1.247681</td>\n",
       "      <td>103.842072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Sime Darby Centre</td>\n",
       "      <td>896 Dunearn Rd, Singapore 589472</td>\n",
       "      <td>1.336644</td>\n",
       "      <td>103.783597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>PoMo</td>\n",
       "      <td>1 Selegie Rd, Singapore 188306</td>\n",
       "      <td>1.300192</td>\n",
       "      <td>103.849220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>LR boulangerie</td>\n",
       "      <td>491 River Valley Rd, #01-02 valley point shopp...</td>\n",
       "      <td>1.293178</td>\n",
       "      <td>103.827194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Tampines Hub</td>\n",
       "      <td>1 Tampines Walk, Singapore 528523</td>\n",
       "      <td>1.353108</td>\n",
       "      <td>103.940361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8667</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Old Chang Kee (Head Office)</td>\n",
       "      <td>2 Woodlands Terrace, Singapore</td>\n",
       "      <td>1.450192</td>\n",
       "      <td>103.805305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8668</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Old Chang Kee @ Sun Plaza</td>\n",
       "      <td>30 Sembawang Dr, #B1-44 Sun Plaza, Singapore</td>\n",
       "      <td>1.448144</td>\n",
       "      <td>103.819983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8669</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Old Chang Kee Bldg</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>1.449830</td>\n",
       "      <td>103.805229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8670</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Old Chang Kee Coldstore</td>\n",
       "      <td>20 Senoko Way, Singapore</td>\n",
       "      <td>1.468055</td>\n",
       "      <td>103.812869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8671</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Ten &amp; Han Trading Pte Ltd</td>\n",
       "      <td>2 Woodlands Terrace, Singapore</td>\n",
       "      <td>1.450047</td>\n",
       "      <td>103.805526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8672 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      store   food  health  restaurant  hospital  lodging  finance   cafe  \\\n",
       "0     False  False   False       False     False    False    False  False   \n",
       "1     False  False   False       False     False    False    False  False   \n",
       "2     False  False   False       False     False    False    False  False   \n",
       "3      True   True   False       False     False    False    False  False   \n",
       "4     False  False   False       False     False    False    False  False   \n",
       "...     ...    ...     ...         ...       ...      ...      ...    ...   \n",
       "8667  False   True   False       False     False    False    False  False   \n",
       "8668  False   True   False       False     False    False    False   True   \n",
       "8669  False  False   False       False     False    False    False  False   \n",
       "8670   True   True   False       False     False    False    False  False   \n",
       "8671  False   True   False        True     False    False    False  False   \n",
       "\n",
       "      convenience_store  clothing_store  ...  police  light_rail_station  \\\n",
       "0                 False           False  ...   False               False   \n",
       "1                 False           False  ...   False               False   \n",
       "2                 False           False  ...   False               False   \n",
       "3                 False           False  ...   False               False   \n",
       "4                 False           False  ...   False               False   \n",
       "...                 ...             ...  ...     ...                 ...   \n",
       "8667              False           False  ...   False               False   \n",
       "8668              False           False  ...   False               False   \n",
       "8669              False           False  ...   False               False   \n",
       "8670              False           False  ...   False               False   \n",
       "8671              False           False  ...   False               False   \n",
       "\n",
       "      city_hall  train_station  natural_feature  subpremise  \\\n",
       "0         False          False            False       False   \n",
       "1         False          False            False       False   \n",
       "2         False          False            False       False   \n",
       "3         False          False            False       False   \n",
       "4         False          False            False       False   \n",
       "...         ...            ...              ...         ...   \n",
       "8667      False          False            False       False   \n",
       "8668      False          False            False       False   \n",
       "8669      False          False            False       False   \n",
       "8670      False          False            False       False   \n",
       "8671      False          False            False       False   \n",
       "\n",
       "                             name  \\\n",
       "0                   Quayside Isle   \n",
       "1               Sime Darby Centre   \n",
       "2                            PoMo   \n",
       "3                  LR boulangerie   \n",
       "4                    Tampines Hub   \n",
       "...                           ...   \n",
       "8667  Old Chang Kee (Head Office)   \n",
       "8668    Old Chang Kee @ Sun Plaza   \n",
       "8669           Old Chang Kee Bldg   \n",
       "8670      Old Chang Kee Coldstore   \n",
       "8671    Ten & Han Trading Pte Ltd   \n",
       "\n",
       "                                      formatted_address       lat         lng  \n",
       "0                        31 Ocean Way, Singapore 098375  1.247681  103.842072  \n",
       "1                      896 Dunearn Rd, Singapore 589472  1.336644  103.783597  \n",
       "2                        1 Selegie Rd, Singapore 188306  1.300192  103.849220  \n",
       "3     491 River Valley Rd, #01-02 valley point shopp...  1.293178  103.827194  \n",
       "4                     1 Tampines Walk, Singapore 528523  1.353108  103.940361  \n",
       "...                                                 ...       ...         ...  \n",
       "8667                     2 Woodlands Terrace, Singapore  1.450192  103.805305  \n",
       "8668       30 Sembawang Dr, #B1-44 Sun Plaza, Singapore  1.448144  103.819983  \n",
       "8669                                          Singapore  1.449830  103.805229  \n",
       "8670                           20 Senoko Way, Singapore  1.468055  103.812869  \n",
       "8671                     2 Woodlands Terrace, Singapore  1.450047  103.805526  \n",
       "\n",
       "[8672 rows x 107 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
